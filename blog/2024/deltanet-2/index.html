<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DeltaNet Explained (Part II) | Songlin Yang</title> <meta name="author" content="Songlin Yang"> <meta name="description" content="An algorithm that parallelizes DeltaNet computation across the sequence length dimension"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/maomao.jpeg?4c57e11efcfd0cc3186dbb6930d90ab5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sustcsonglin.github.io/blog/2024/deltanet-2/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "DeltaNet Explained (Part II)",
      "description": "An algorithm that parallelizes DeltaNet computation across the sequence length dimension",
      "published": "December 3, 2024",
      "authors": [
        {
          "author": "Songlin Yang",
          "authorURL": "https://sustcsonglin.github.io/",
          "affiliations": [
            {
              "name": "MIT CSAIL",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Songlin </span>Yang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>DeltaNet Explained (Part II)</h1> <p>An algorithm that parallelizes DeltaNet computation across the sequence length dimension</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#parallel-scan-for-deltanet-a-failed-attempt">Parallel Scan for DeltaNet: A Failed Attempt</a></div> <div><a href="#wy-representation-for-deltanet">WY representation for DeltaNet</a></div> </nav> </d-contents> <p><strong>This blog post series accompanies our NeurIPS ‘24 paper - <a href="https://arxiv.org/abs/2406.06484" rel="external nofollow noopener" target="_blank">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a>. You can find the implementation <a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py" rel="external nofollow noopener" target="_blank">here</a> and the presentation slides <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf" rel="external nofollow noopener" target="_blank">here</a>.</strong></p> <ol> <li><a href="/blog/2024/deltanet-1/">Part I - The Model</a></li> <li><a href="#">Part II - The Algorithm</a></li> </ol> <h2 id="parallel-scan-for-deltanet-a-failed-attempt">Parallel Scan for DeltaNet: A Failed Attempt</h2> <h3 id="from-delta-updates-to-matrix-multiplication-form">From Delta Updates to Matrix Multiplication Form</h3> <p>Let’s start with DeltaNet’s original state update equation:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top\] <p>To transform this into a matrix multiplication form, let’s expand the equation step by step:</p> \[\begin{align*} \mathbf{S}_{t} &amp;= \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} - \beta_t \mathbf{S}_{t-1} \mathbf{k}_t \mathbf{k}_t^\top + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal) + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \end{align*}\] <p>For simplicity, let’s denote:</p> <ul> <li>\(\mathbf{M}_t = \mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal\) as our transition matrix</li> <li>\(\mathbf{X}_t = \beta_t \mathbf{v}_t \mathbf{k}_t^\top\) as our update term</li> </ul> <p>Then our update becomes:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1}\mathbf{M}_t + \mathbf{X}_t \in \mathbb{R}^{d\times d}\] <h3 id="defining-the-associative-operator">Defining the Associative Operator</h3> <p>This form matches exactly with the first-order recurrence shown in equation (1.5) from <em>Prefix Sums and Their Applications</em><d-cite key="Blelloch1990PrefixSA"></d-cite>, where matrix multiplication (⊗) and matrix addition (⊕) serve as our binary operators. Both operators satisfy the required properties:</p> <ol> <li>Matrix addition is associative: \((A + B) + C = A + (B + C)\)</li> <li>Matrix multiplication is associative: \((AB)C = A(BC)\)</li> <li>Matrix multiplication distributes over addition: \(A(B + C) = AB + AC\)</li> </ol> <p>Following the framework, we define our state pairs as:</p> \[c_t = [\mathbf{M}_t, \mathbf{X}_t] = [\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal, \beta_t \mathbf{v}_t \mathbf{k}_t^\top]\] <p>And our associative operator • that combines these pairs:</p> \[c_i \bullet c_j = [\mathbf{M}_i\mathbf{M}_j, \mathbf{M}_j\mathbf{X}_i + \mathbf{X}_j]\] <p>This operator definition preserves the temporal dependencies of our updates - when we combine two steps, the earlier update term \(\mathbf{X}_i\) must be transformed by the later transition matrix \(\mathbf{M}_j\), while the later update term \(\mathbf{X}_j\) remains unchanged.</p> <h3 id="parallel-scan">Parallel Scan</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/scan.png" alt="示例图片" style="width: 99%"> </div> </div> <p>With this associative operator, we can use parallel scan to compute all states in parallel. The algorithm works in two phases:</p> <h5 id="sweep-down-phase">Sweep-Down Phase</h5> <p>First, we compute partial results in parallel by combining adjacent pairs:</p> <p>For steps 0 and 1, we compute:</p> \[c_1 = c_0 \bullet c_1 = [\mathbf{M}_0\mathbf{M}_1, \mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1]\] <p>Similarly for steps 2 and 3:</p> \[c_3 = c_2 \bullet c_3 = [\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]\] <p>Then combine these results:</p> \[c_{1:3} = c_{1} \bullet c_{3} = [\mathbf{M}_0\mathbf{M}_1\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_2\mathbf{M}_3(\mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1) + \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]\] <h5 id="sweep-up-phase">Sweep-Up Phase</h5> <p>In this phase, we use our partial results to compute intermediate states:</p> \[c_2 = c_1 \bullet c_2 = [\mathbf{M}_1\mathbf{M}_2, \mathbf{M}_2\mathbf{X}_1 + \mathbf{X}_2]\] <p>This parallelization transforms DeltaNet’s sequential state updates into an efficient parallel computation, reducing the sequential dependency chain from \(\mathbf{O}(L)\) to \(\mathcal{O}(\log L)\) steps while maintaining mathematical equivalence.</p> <h3 id="whats-wrong-with-parallel-scan-for-deltanet">What’s Wrong with Parallel Scan for DeltaNet?</h3> <p>Despite parallelizability, parallel scan for DeltaNet faces two major challenges: computational complexity and memory requirements.</p> <p>The first issue lies in the <strong>time complexity</strong>. For DeltaNet, parallel scan yields \(\mathcal{O}(L\log L d^3)\) complexity due to the cubic cost of matrix multiplication when treating \(\mathbf{M}_t\) as dense matrices. At first glance, we might think we can leverage the identity-plus-low-rank structure of \(\mathbf{M}_t\) for acceleration. Let’s work through this carefully.</p> <p>When multiplying two adjacent matrices, we get:</p> \[\begin{align*} (\mathbf{I}-\beta_0 \mathbf{k}_0 \mathbf{k}_0^\top)(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) &amp;= \mathbf{I}(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) \\ &amp;= (\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 \mathbf{k}_0^\top \mathbf{k}_1 \mathbf{k}_1^\top \\ &amp;= \mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top \end{align*}\] <p>This computation reduces the complexity from \(\mathcal{O}(d^3)\) to \(\mathcal{O}(d^2)\) by leveraging the identity-plus-low-rank structure - we only need to compute vector inner products \((\mathbf{k}_0^\top \mathbf{k}_1)\) and outer products between vectors. Similarly for the next pair:</p> \[\begin{align*} (\mathbf{I}-\beta_2 \mathbf{k}_2 \mathbf{k}_2^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top) &amp;= \mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top \end{align*}\] <p>When we try to combine these results to compute larger spans like \(c_{1:4}\), the multiplication becomes increasingly complex. We need to multiply:</p> \[(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top)\] <p>Each term in the first bracket must multiply with each term in the second bracket, leading to a quadratic growth in the number of terms. This quickly becomes unmanageable due to the combinatorial explosion of terms. This suggests we might be better off just treating it as dense matrix multiplication.</p> <p>The second major issue is <strong>space complexity</strong>. Parallel scan requires materializing all intermediate d×d matrices at each step to high-bandwidth memory (HBM). For linear RNNs with matrix-valued states, this materialization becomes prohibitively expensive (\(\mathcal{O}(Ld^2)\)). While recurrent computation can avoid such materialization, parallel scan offers no apparent workaround unless all states fit into SRAM, as implemented in Mamba’s hardware-aware selective scan algorithm that eliminates the need for materialization. However, this approach imposes limitations on state size - too large a state leads to out-of-shared-memory issues. Given that I/O costs dominate this computation, parallel scan may become undesirable in practice. As noted in recent discussions:</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Dear parallel-scan people.<br><br>Q(t) and K(t) are 1xD and V(t) is 1xC, with D and T large, and I want to compute<br><br>Y(t) = Q(t)M(t)<br><br>with<br><br>M(0)=0<br>M(t+1) = M(t) + K(t)^T V(t)<br><br>A naive non-parallel scan approach is O(T) in time but I do not have to store any DxD<br><br>1/2</p>— François Fleuret (@francoisfleuret) <a href="https://twitter.com/francoisfleuret/status/1793016689589625263?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">May 21, 2024</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">The key idea is to only materialize chunk-level hidden states, using matmuls to calculate outputs based on the query, key, and value matrices and chunk-level hidden states. This method avoids materializing the hidden state for every single timestep.</p>— Songlin Yang ✈️ NeurIPS '24 (@SonglinYang4) <a href="https://twitter.com/SonglinYang4/status/1793029555277697379?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">May 21, 2024</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>Here I previously discussed the chunkwise algorithm - another type of associative scan that offers improved memory efficiency and better utilization of tensor cores by enabling more matrix multiplication operations (for a detailed analysis, see <d-cite key="yang_gated_2023"></d-cite>). Given these advantages, developing a chunkwise training algorithm for DeltaNet that maintains quadratic complexity with respect to \(d\) while preserving memory efficiency would be highly valuable.</p> <h2 id="wy-representation-for-deltanet">WY representation for DeltaNet</h2> <p>Stay Tuned!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-12-03-delta.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Songlin Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>