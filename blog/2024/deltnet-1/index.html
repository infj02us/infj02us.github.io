<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DeltaNet Explained (Part I) | Songlin Yang</title> <meta name="author" content="Songlin Yang"> <meta name="description" content="A gentle and comprehensive introduction to the DeltaNet"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/maomao.jpeg?4c57e11efcfd0cc3186dbb6930d90ab5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sustcsonglin.github.io/blog/2024/deltnet-1/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "DeltaNet Explained (Part I)",
      "description": "A gentle and comprehensive introduction to the DeltaNet",
      "published": "December 3, 2024",
      "authors": [
        {
          "author": "Songlin Yang",
          "authorURL": "https://sustcsonglin.github.io/",
          "affiliations": [
            {
              "name": "MIT CSAIL",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Songlin </span>Yang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>DeltaNet Explained (Part I)</h1> <p>A gentle and comprehensive introduction to the DeltaNet</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#linear-attention-as-rnn">Linear attention as RNN</a></div> <ul> <li><a href="#what-is-linear-attention">What is linear attention?</a></li> <li><a href="#no-free-lunch-key-limitations-of-linear-attention">No Free Lunch: Key Limitations of Linear Attention</a></li> </ul> <div><a href="#deltanet-linear-attention-with-delta-rule">DeltaNet: Linear Attention with Delta Rule</a></div> <ul> <li><a href="#what-is-delta-rule">What is Delta Rule?</a></li> <li><a href="#what-is-deltanet">What is DeltaNet?</a></li> <li><a href="#deltanet-as-an-strong-rnn-in-context-learner">DeltaNet as an Strong RNN In-context Learner</a></li> <li><a href="#why-is-deltanet-superior-at-in-context-retrieval-than-linear-attention">Why is DeltaNet Superior at In-context Retrieval than Linear Attention?</a></li> </ul> </nav> </d-contents> <p>This is a blogpost accompanying our recent NeurIPS ‘24 publication - <a href="https://arxiv.org/abs/2406.06484" rel="external nofollow noopener" target="_blank">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a>. (<a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py" rel="external nofollow noopener" target="_blank">code</a>, <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf" rel="external nofollow noopener" target="_blank">slides</a>)</p> <h2 id="linear-attention-as-rnn">Linear attention as RNN</h2> <h3 id="what-is-linear-attention">What is linear attention?</h3> <p>The vanilla softmax attention mechanism, though powerful, suffers from quadratic complexity in sequence length. Let’s see how linear attention addresses this issue by starting with the standard softmax attention:</p> \[\begin{aligned} \mathbf{O} &amp;= \mathrm{softmax}(\mathbf{Q}\mathbf{K}^T \odot \mathbf{M})\mathbf{V} &amp;&amp;&amp;&amp; \mathrm{Parallel\ training} \\ \mathbf{o_t} &amp;= \sum_{j=1}^t \frac{\exp(\mathbf{q}_t^T \mathbf{k}_j)}{\sum_{l=1}^t\exp(\mathbf{q_t}^T \mathbf{k}_l)}\mathbf{v}_j &amp;&amp;&amp;&amp; \mathrm{Iterative\ inference} \end{aligned}\] <p>Here, \(\mathbf{M}\) is the causal mask for autoregressive modeling. The key observation is that the softmax operation is the main source of complexity - both computationally and in terms of memory usage.</p> <p>What linear attention<d-cite key="katharopoulos2020transformers"></d-cite> does is simply removing the softmax operator:</p> \[\begin{aligned} \mathbf{O} &amp;= (\mathbf{Q}\mathbf{K}^T \odot \mathbf{M})\mathbf{V} &amp;&amp;&amp;&amp;&amp; \mathrm{Parallel\ training} \\ \mathbf{o_t} &amp;= \sum_{j=1}^t (\mathbf{q}_t^T \mathbf{k}_j) \mathbf{v}_j^T &amp;&amp;&amp;&amp;&amp; \mathrm{Iterative\ inference} \end{aligned}\] <p>By associativity, we can restructure the inference computation as:</p> \[\begin{aligned} \mathbf{o_t} &amp;= \sum_{j=1}^t \mathbf{q}_t^\top (\mathbf{k}_j\mathbf{v}_j^\top) \\ &amp;=\mathbf{q}_t^\top\sum_{j=1}^t\mathbf{k}_j\mathbf{v}_j^\top \\ &amp;= (\sum_{j=1}^t\mathbf{v}_j\mathbf{k}_j^\top)\mathbf{q}_t \end{aligned}\] <p>This restructuring reveals something interesting. Let’s define a state matrix \(\mathbf{S}_t = \sum_{j=1}^t\mathbf{v}_j\mathbf{k}_j^\top\). Then the computation can be expressed as:</p> \[\mathbf{S}_t = \mathbf{S}_{t-1} + \mathbf{v}_t\mathbf{k}_t^\top \in \mathbb{R}^{d\times d}, \quad \mathbf{o}_t = \mathbf{S}_t \mathbf{q}_t \in \mathbb{R}^{d}\] <p>where \(d\) is (head) dimension. Now we can see that linear attention is essentially a linear RNN with a <em>matrix-valued</em> state \(\mathbf{S}\) that accumulates key-value outer products, facilitating state expansion in a hardware-efficient manner.</p> <details> <summary> Why do we want state expansion?</summary> Traditionally, RNN's hidden dimension is often the same (or of the same magnitude) as the input dimension, due to the expensive matrix-multiply-based state update. However, RNN solely relies on the recurrent state to remember the entire history and state size tends to be the bottleneck to remember sufficient amount of information, especially in retrieval tasks. We've been observing a substantial amount of research investigating hardware-efficient state expansion since Mamba1<d-cite key="Gu2023MambaLS"></d-cite> explicitly pointed it out, and linear attention styled outer-product-based update has proven to be optimal in terms of efficiently scaling state up (Mamba2<d-cite key="mamba2"></d-cite> also adopts this strategy!). In our previous HGRN2 work<d-cite key="qin_hgrn2_2024"></d-cite>, we investigated different approaches for state expansion, and the outer product based mechanism has proven to be both performant and scalable. </details> <p>This state-space view not only provides theoretical insights but also enables efficient incremental computation - we only need to store and update \(\mathbf{S}_t\) instead of all previous key-value pairs. This formulation reduces the time complexity from \(O(L^2)\) to \(O(L)\) for autoregressive inference, and the space complexity from \(O(L)\) to \(O(1)\), making it particularly attractive in two scenarios:</p> <ul> <li> <p><strong>Long sequence modeling</strong> where quadratic complexity could be a bottleneck.</p> </li> <li> <p>Generation where computation is <strong>memory-bound</strong> and the removal of KV cache significantly improves <strong>inference latency</strong>.</p> </li> </ul> <h3 id="no-free-lunch-key-limitations-of-linear-attention">No Free Lunch: Key Limitations of Linear Attention</h3> <p>Unfortunately, there is no free lunch. The fixed-size state matrix in linear attention means it cannot perfectly preserve all historical information, making exact retrieval particularly challenging.</p> <p>More formally, linear attention implements a key-value associative memory. Assuming all keys are normalized to unit length, we can analyze this from the perspective of tensor-product representation. When we try to retrieve a value associated with a specific key \(k_j\), we get:</p> \[\begin{aligned} S &amp;= \sum_{i=1}^t v_ik_i^T \\ S k_j &amp;= \sum_{i=1}^t v_i (k_i^Tk_j) \\ &amp;= v_j + \underbrace{\sum_{i\neq j} (k_i^Tk_j)v_i}_{\text{retrieval error}} \end{aligned}\] <p>To minimize the retrieval error term, we need \(k_i^T k_j = 0\) for all \(i\neq j\) - in other words, all keys should be orthogonal to each other. However, this reveals a fundamental limitation: in a \(d\)-dimensional space, you can only have at most \(d\) orthogonal vectors. This explains why increasing head dimension helps - it provides more “room” in the vector space for storing distinct key-value pairs!</p> <p>This theoretical limitation manifests in practice. Despite recent advances in gated variants of linear attention (e.g., GLA<d-cite key="yang_gated_2023"></d-cite>, Mamba<d-cite key="Gu2023MambaLS"></d-cite>) significantly narrowing the performance gap with standard attention,</p> <details> <summary>Enhance linear attention using gating mechanism</summary> <p> Given the close relationship between linear attention and RNN, it is no wonder that researchers want to enhance linear attention with the (forgetting) gating mechanisms, which has been shown unreasonably effective in nonlinear RNN<d-cite key="unreasonable-forget-gate"></d-cite> and linear RNN<d-cite key="HGRN"></d-cite>: </p> <p> \[\mathbf{S}_t = \mathbf{G}_t \odot \mathbf{S}_{t-1} + \mathbf{v}_t\mathbf{k}_t^T\] </p> <p> with different structured parameterization for \(\mathbf{G}_t \in \mathbb{R}^{d\times d}\) for parameter efficiency, often with outer product structure. Different models have proposed various ways to structure this gating matrix: </p> <p> For Decaying Fast weight<d-cite key="mao_fine-tuning_2022"></d-cite>: \[\mathbf{G}_t = \mathbf{\beta_t} \mathbf{\alpha_t}^T\] </p> <p> For GLA<d-cite key="yang_gated_2023"></d-cite>: \[\mathbf{G}_t = \mathbf{1} \mathbf{\alpha_t}^T\] </p> <p> For Mamba1<d-cite key="Gu2023MambaLS"></d-cite>: \[\mathbf{G}_t = \exp(-(\mathbf{\Delta_t} \mathbf{1}^T) \odot \exp(A))\] </p> <p> For Mamba2<d-cite key="mamba2"></d-cite>: \[\mathbf{G}_t = \gamma_t \mathbf{1}\mathbf{1}^T\] </p> <p> Cf. Table 1 of GLA<d-cite key="yang_gated_2023"></d-cite> for a summarization. </p> </details> <p>in-context retrieval and exact copying remain challenging. This has been both empirically observed and theoretically proven in recent works.</p> <h2 id="deltanet-linear-attention-with-delta-rule">DeltaNet: Linear Attention with Delta Rule</h2> <h3 id="what-is-delta-rule">What is Delta Rule?</h3> <p>The Delta Rule<d-cite key="widrow_adaptive_1988"></d-cite> is a fundamental error-correction learning principle in neural networks. Its core idea is beautifully simple: adjust the model’s parameters based on the difference (delta) between what we want (target) and what we actually get (prediction).</p> <p>To understand this intuitively, imagine teaching a child to aim at a target. If they shoot too far to the left, you’d tell them to adjust right; too far right, adjust left. The size of the adjustment depends on how far they missed - a concept directly reflected in the Delta Rule.</p> <details> <summary>Click to expand Delta Rule code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">delta_rule</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Simple delta rule implementation
    x: input features (N samples by D features)
    y: target values (N samples)
    </span><span class="sh">"""</span>
    <span class="c1"># Initialize weights
</span>    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="c1"># Train
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
            <span class="c1"># Forward pass
</span>            <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span>
            
            <span class="c1"># Compute error
</span>            <span class="n">error</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">pred</span>
            
            <span class="c1"># Update weights
</span>            <span class="n">w</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">error</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            
    <span class="k">return</span> <span class="n">w</span>

<span class="c1"># Example usage
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Generate toy data
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 100 samples, 3 features
</span>    <span class="n">true_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">true_w</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    
    <span class="c1"># Train
</span>    <span class="n">w</span> <span class="o">=</span> <span class="nf">delta_rule</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">True weights:</span><span class="sh">"</span><span class="p">,</span> <span class="n">true_w</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Learned weights:</span><span class="sh">"</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span></code></pre></figure> </details> <h3 id="what-is-deltanet">What is DeltaNet?</h3> <p>DeltaNet<d-cite key="schlag_linear_2021"></d-cite> elegantly applies this error-correction principle to linear attention. Instead of simply accumulating information additively, it updates its state based on prediction errors:</p> \[S_{t} = S_{t-1} - \beta_t(S_{t-1} k_t - v_t)k_t^T = S_{t-1} - \beta_t S_{t-1} k_t k_t^T + \beta_t v_t k_t^T\] <p>The parallel to the Delta Rule becomes clear when we break down the components:</p> <ul> <li>\(\beta_t\) acts as the learning rate</li> <li>\(k_t\) is the input data</li> <li>\(v_t\) is the target</li> <li>\(S_{t-1} k_t\) is our current prediction</li> </ul> <p>There’s another intuitive way to understand this update rule. Think of \(S_{t-1}k_t\) as retrieving the “old value” associated with the current key \(k_t\) from memory. When we encounter a newly associated value \(v_t\) for the same key, rather than blindly overwriting, we make a careful update:</p> \[v_t^{\text{new}} = (1-\beta_t) v_t^{\text{old}} + \beta_t v_t, \qquad S_t = S_{t-1} - \underbrace{v_t^{\text{old}} k_t^T}_{\text{erase}} + \underbrace{v_t^{\text{new}} k_t^T}_{\text{write}}\] <p>where \(v_t^{\text{new}}\) is a learned combination of the old and current values, controlled by a dynamic \(\beta_t \in (0,1)\). When \(\beta_t=0\), the memory content remains intact, and when \(\beta_t=1\), we completely replace the old associated value with the new one.</p> <h3 id="deltanet-as-an-strong-rnn-in-context-learner">DeltaNet as an Strong RNN In-context Learner</h3> <p>DeltaNet was originally designed to enhance associative recall performance. MQAR (Multi-Query Associative Recall)<d-cite key="zoology"></d-cite> is a recent popular synthetic benchmark aimed at measuring the in-context recall ability for subquadratic models.</p> <p>The MQAR task works as follows: Each letter is associated with a number, and the model is asked to correctly recall the number associated with each letter in a query sequence, where multiple queries for the same association are allowed.</p> <p>For example, given the input:</p> <p><code class="language-plaintext highlighter-rouge">A 4 B 3 C 6 F 1 E 2 → A ? C ? F ? E ? B ?</code></p> <p>The format consists of:</p> <ol> <li>Key-Value pairs (before the arrow): Letters paired with their corresponding numbers</li> <li>Query sequence (after the arrow): Letters whose associated numbers need to be recalled</li> </ol> <p>The correct output for this example would be: 4, 6, 1, 2, 3</p> <p>This benchmark effectively tests a model’s ability to store and retrieve multiple associations accurately within its context window, making it particularly valuable for evaluating the recall capabilities of subquadratic models. While conventional gated convolution and recurrent models generally underperform in this domain, DeltaNet demonstrates notably strong performance in such synthetic retrieval tasks in our experiments.</p> <div class="row justify-content-center"> <div class="col-6"> <img class="img-fluid" style="background-color: white; padding: 20px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" src="/assets/img/blog/deltanet/mqar-1.png"> </div> </div> <div class="caption"> The hardest setting from the original Zoology paper </div> <p>This initial success was particularly exciting - the perfect performance on MQAR caught us by surprise. What makes this result especially promising is that MQAR performance is closely correlated with “Associative-Recall-Hit” in real-world language modeling tasks. This connection provided us with strong motivation to explore scaling up DeltaNet.</p> <p>After all, we wouldn’t want to invest in scaling up an arbitrary model without compelling evidence of its potential. MQAR serves as an excellent proxy metric, giving us confidence that a model’s performance will translate well to real-world language modeling tasks.</p> <p>We’ve also conducted experiments on MAD<d-cite key="poli_mechanistic_2024"></d-cite>, another more comprehensive benchmark than MQAR that is also motivated to test new architecture’s capacities, and the results are summarized below:</p> <table> <thead> <tr> <th>Model</th> <th>Compress</th> <th>Fuzzy Recall</th> <th>In-Context Recall</th> <th>Memorize</th> <th>Noisy Recall</th> <th>Selective Copy</th> <th>Average</th> </tr> </thead> <tbody> <tr> <td>Transformer</td> <td>51.6</td> <td>29.8</td> <td>94.1</td> <td>85.2</td> <td>86.8</td> <td>99.6</td> <td>74.5</td> </tr> <tr> <td>Hyena</td> <td>45.2</td> <td>7.9</td> <td>81.7</td> <td>89.5</td> <td>78.8</td> <td>93.1</td> <td>66.0</td> </tr> <tr> <td>Multihead Hyena</td> <td>44.8</td> <td>14.4</td> <td>99.0</td> <td>89.4</td> <td>98.6</td> <td>93.0</td> <td>73.2</td> </tr> <tr> <td>Mamba</td> <td>52.7</td> <td>6.7</td> <td>90.4</td> <td>89.5</td> <td>90.1</td> <td>86.3</td> <td>69.3</td> </tr> <tr> <td>GLA</td> <td>38.8</td> <td>6.9</td> <td>80.8</td> <td>63.3</td> <td>81.6</td> <td>88.6</td> <td>60.0</td> </tr> <tr> <td>DeltaNet</td> <td>42.2</td> <td>35.7</td> <td>100</td> <td>52.8</td> <td>100</td> <td>100</td> <td>71.8</td> </tr> </tbody> </table> <p>where DeltaNet demonstrates its strong in-context recall capacities. In the next post, we’ll explore a beautiful algorithm that parallelizes DeltaNet across sequence length. But first, let’s build some intuition about why DeltaNet is particularly well-suited for in-context recall tasks.</p> <h3 id="why-is-deltanet-superior-at-in-context-retrieval-than-linear-attention">Why is DeltaNet Superior at In-context Retrieval than Linear Attention?</h3> <p>The key to DeltaNet’s superiority lies in its choice of the L2 (squared error) loss function:</p> \[L(S) = \frac{1}{2}|S k_t - v_t|^2_F\] <p>This quadratic loss function has several crucial advantages. First, it directly measures how far our predictions are from the truth in terms of Euclidean distance. Second, because it’s squared, it penalizes large errors much more heavily than small ones, encouraging the model to fix significant mistakes first. When we perform gradient descent on this loss, we get:</p> \[\begin{aligned} S_t &amp;= S_{t-1} - \eta_t \nabla L(S) \\ &amp;= S_{t-1} - \eta_t (S_{t-1} k_t - v_t) k_t^T \end{aligned}\] <p>where $\eta_t = \beta_t$ gives us the DeltaNet update rule. This update rule behaves like an attentive student who adjusts their learning based on how big their mistakes are. The L2 loss creates an error-proportional correction term $(S_{t-1} k_t - v_t)$, meaning the model makes larger updates when predictions are far off and smaller, more precise adjustments as predictions improve. Through $\beta_t$, DeltaNet can also adapt its learning rate based on confidence in each update.</p> <p>In contrast, linear attention uses a simpler linear loss:</p> \[L(S) = -\langle S k_t, v_t \rangle_F\] <p>Leading to the update rule:</p> \[\begin{aligned} S_t &amp;= S_{t-1} - \eta_t \nabla L(S) \\ &amp;= S_{t-1} + \eta_t v_t k_t^T \end{aligned}\] <p>where $\eta_t = 1$ gives us the linear attention update rule. This linear loss has fundamental limitations. It only considers the directional alignment between predictions and targets, missing out on the benefits of squared error minimization. Its update doesn’t scale with error magnitude - it’s like a student who makes the same size adjustment regardless of how wrong they were. This can be particularly problematic for tasks requiring precise memory retrieval, where we want to rapidly correct large errors while making careful refinements for small ones.</p> <p>The L2 loss in DeltaNet makes it especially strong at MQAR tasks, where precise key-value associations are crucial. By directly optimizing for prediction accuracy through squared error minimization, it can build and maintain more accurate memory representations. This sophisticated error correction mechanism, driven by the properties of L2 loss, makes DeltaNet particularly valuable for applications requiring precise context recall.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-12-03-delta.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"sustcsonglin/sustcsonglin.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Songlin Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>