<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DeltaNet Explained (Part III) | Songlin Yang</title> <meta name="author" content="Songlin Yang"> <meta name="description" content="Modernize DeltaNet neural architecture"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/maomao.jpeg?4c57e11efcfd0cc3186dbb6930d90ab5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sustcsonglin.github.io/blog/2024/deltanet-3/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "DeltaNet Explained (Part III)",
      "description": "Modernize DeltaNet neural architecture",
      "published": "December 3, 2024",
      "authors": [
        {
          "author": "Songlin Yang",
          "authorURL": "https://sustcsonglin.github.io/",
          "affiliations": [
            {
              "name": "MIT CSAIL",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Songlin </span>Yang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>DeltaNet Explained (Part III)</h1> <p>Modernize DeltaNet neural architecture</p> </d-title><d-byline></d-byline><d-article> <p><strong>This blog post series accompanies our NeurIPS ‘24 paper - <a href="https://arxiv.org/abs/2406.06484" rel="external nofollow noopener" target="_blank">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a></strong> (w/ <a href="https://berlino.github.io/" rel="external nofollow noopener" target="_blank">Bailin Wang</a>, <a href="https://yzhang.site/" rel="external nofollow noopener" target="_blank">Yu Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/yikang-shen/" rel="external nofollow noopener" target="_blank">Yikang Shen</a> and <a href="https://people.csail.mit.edu/yoonkim/" rel="external nofollow noopener" target="_blank">Yoon Kim</a>). <strong>You can find the implementation <a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py" rel="external nofollow noopener" target="_blank">here</a> and the presentation slides <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf" rel="external nofollow noopener" target="_blank">here</a>.</strong></p> <ol> <li><a href="/blog/2024/deltanet-1/">Part I - The Model</a></li> <li><a href="/blog/2024/deltanet-2/">Part II - The Algorithm</a></li> <li><a href="#">Part III - The Neural Architecture</a></li> </ol> <h2 id="deltanet-architecture-design">DeltaNet architecture design</h2> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/delta-net-arch.png" alt="示例图片" style="width: 99%"> <figcaption style="margin-top: 10px; color: #666;"> DeltaNet architecture </figcaption> </div> </div> <p>We modernize the original DeltaNet with several key architectural improvements. The overall model structure follows Llama’s design, alternating between a token mixer (DeltaNet instead of self-attention) and a channel mixer (SwiGLU). Within the DeltaNet layer, queries and keys share the same processing pipeline: Linear → ShortConv → SiLU → L2Norm, while values follow Linear → ShortConv → SiLU, and the beta term is computed through Linear → Sigmoid. Let’s examine the motivation behind each of these design choices.</p> <h3 id="normalization-on-transition-matrices">Normalization on transition matrices</h3> <p>A crucial aspect of DeltaNet’s architecture is the normalization of key vectors. This isn’t just a technical detail - it’s fundamental to the model’s stability and effectiveness. Consider DeltaNet’s core equation:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t\mathbf{k}_t^\top) + \mathbf{v}_t\mathbf{k}_t^\top\] <p>The stability of this recurrent system depends on the eigenvalues of its transition matrix \((\mathbf{I} - \beta_t \mathbf{k}_t\mathbf{k}_t^\top)\). This matrix has an elegant spectral structure:</p> <ul> <li>The eigenvalue in the direction of \(\mathbf{k}_t\) is \(1 - \beta_t\|\mathbf{k}_t\|^2\)</li> <li>All directions perpendicular to \(\mathbf{k}_t\) have eigenvalue 1</li> </ul> <p>For stable updates, we need all eigenvalues to have magnitude \(\leq 1\). Given \(0 \leq \beta_t \leq 1\), this requires \(\|\mathbf{k}_t\|^2 \leq 2\). While the original DeltaNet used L1 normalization, we found L2 normalization offers both better empirical performance and a more intuitive geometric interpretation: when \(\beta_t = 1\) and \(\|\mathbf{k}_t\|_2 = 1\), the matrix \(\mathbf{I} - \mathbf{k}_t\mathbf{k}_t^\top\) becomes a projection matrix that selectively erases information in the direction of \(\mathbf{k}_t\) while preserving all other directions.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/projection.png" alt="示例图片" style="width: 67%"> </div> </div> <p>We also found that L2 normalization on queries improves performance, possibly related to the success of QK-normalization in recent attention models.</p> <h3 id="activation-function-choice">Activation Function Choice</h3> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/silu.png" alt="示例图片" style="width: 50%"> </div> </div> <p>Our experiments show SiLU activation performs slightly better than alternatives like ReLU and 1+ELU. This presents an interesting contrast with traditional linear attention mechanisms, where activation functions are typically chosen to ensure positive attention scores through positive feature maps. SiLU’s ability to produce negative values actually appears beneficial in our case. This aligns with recent findings from works like Differential Transformers that demonstrate the value of allowing negative attention scores - suggesting that restricting attention to be strictly positive might be unnecessarily limiting.</p> <h3 id="the-critical-role-of-short-convolution">The Critical Role of Short Convolution</h3> <p>Short convolution has emerged as a fundamental component in recent efficient sequence models (Mamba, xLSTM, MetaLA). Its importance in DeltaNet can be understood through the lens of the Neural Turing Machine (NTM), which identified two fundamental types of memory addressing:</p> <ol> <li> <strong>Content-based addressing</strong>: DeltaNet’s core mechanism, finding information based on key similarity</li> <li> <strong>Position-based addressing</strong>: Accessing information based on exact positions</li> </ol> <p>DeltaNet’s core operation centers on content-based addressing, implemented through its state update rule: \(\mathbf{S}_t = \mathbf{S}_{t-1}(\mathbf{I} - \beta_t\mathbf{k}_t\mathbf{k}_t^\top) + \beta_t\mathbf{v}_t\mathbf{k}_t^\top\). This update mechanism allows DeltaNet to locate and modify information based on key similarity, analogous to how NTM’s content addressing locates memory locations with similar content. While the sequential nature of these updates makes the system inherently order-sensitive (as each update affects the state seen by subsequent operations), the primary mechanism still operates through content similarity rather than explicit positional information.</p> <p>However, many patterns in sequence processing require precise positional relationships. The NTM recognized this need and introduced convolution-based position addressing as a complementary mechanism to its content addressing. This allowed the NTM to shift attention by exact positions relative to its current focus, enabling operations like “move one step left” or “copy the next three elements.”</p> <p>Short convolution in DeltaNet serves a similar purpose - it provides deterministic access to recent positions, independent of content. This position-based addressing is essential for tasks like forming induction heads within a single layer, where a model needs to identify and copy patterns from specific relative positions (like “A B A B A …”). While DeltaNet’s content-based mechanism could theoretically learn such patterns, having direct positional access through convolution makes this much more reliable.</p> <p>Think of content addressing as finding information by what it contains (like looking up a topic in a book’s index) versus position addressing as finding information by where it sits (like knowing exactly which page to turn to). Both capabilities are necessary: content addressing for flexible, semantic relationships, and position addressing for precise, structural patterns. This duality, first recognized in the NTM’s design, remains crucial for modern architectures like DeltaNet.</p> <h3 id="hybrid-models-combining-deltanet-with-attention">Hybrid Models: Combining DeltaNet with Attention</h3> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/hybrid.png" alt="示例图片" style="width: 99%"> <figcaption style="margin-top: 10px; color: #666;"> Left: Hybrid sliding window attention and DeltaNet model. Right: Hybrid global attention and DeltaNet model. </figcaption> </div> </div> <p>Stay tuned!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-12-03-delta.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Songlin Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>