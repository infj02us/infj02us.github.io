<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DeltaNet Explained (Part III) | Songlin Yang</title> <meta name="author" content="Songlin Yang"> <meta name="description" content="Modernize DeltaNet neural architecture"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/maomao.jpeg?4c57e11efcfd0cc3186dbb6930d90ab5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sustcsonglin.github.io/blog/2024/deltanet-3/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "DeltaNet Explained (Part III)",
      "description": "Modernize DeltaNet neural architecture",
      "published": "December 3, 2024",
      "authors": [
        {
          "author": "Songlin Yang",
          "authorURL": "https://sustcsonglin.github.io/",
          "affiliations": [
            {
              "name": "MIT CSAIL",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Songlin </span>Yang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>DeltaNet Explained (Part III)</h1> <p>Modernize DeltaNet neural architecture</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#deltanet-architecture-design">DeltaNet architecture design</a></div> <ul> <li><a href="#normalization-on-queries-and-keys">Normalization on Queries and Keys</a></li> <li><a href="#normalization-on-outputs">Normalization on Outputs</a></li> <li><a href="#activation-function-choice">Activation Function Choice</a></li> <li><a href="#short-convolution">Short Convolution</a></li> <li><a href="#experimental-results">Experimental Results</a></li> </ul> <div><a href="#hybrid-models-combining-deltanet-with-attention">Hybrid Models: Combining DeltaNet with Attention</a></div> </nav> </d-contents> <p><strong>This blog post series accompanies our NeurIPS ‘24 paper - <a href="https://arxiv.org/abs/2406.06484" rel="external nofollow noopener" target="_blank">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a></strong> (w/ <a href="https://berlino.github.io/" rel="external nofollow noopener" target="_blank">Bailin Wang</a>, <a href="https://yzhang.site/" rel="external nofollow noopener" target="_blank">Yu Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/yikang-shen/" rel="external nofollow noopener" target="_blank">Yikang Shen</a> and <a href="https://people.csail.mit.edu/yoonkim/" rel="external nofollow noopener" target="_blank">Yoon Kim</a>). <strong>You can find the implementation <a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py" rel="external nofollow noopener" target="_blank">here</a> and the presentation slides <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf" rel="external nofollow noopener" target="_blank">here</a>.</strong></p> <ol> <li><a href="/blog/2024/deltanet-1/">Part I - The Model</a></li> <li><a href="/blog/2024/deltanet-2/">Part II - The Algorithm</a></li> <li><a href="#">Part III - The Neural Architecture</a></li> </ol> <h2 id="deltanet-architecture-design">DeltaNet architecture design</h2> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/delta-net-arch.png" alt="示例图片" style="width: 99%"> <figcaption style="margin-top: 10px; color: #666;"> DeltaNet architecture </figcaption> </div> </div> <p>In this final post, we explore our modernization of DeltaNet’s architecture. While maintaining the core delta rule mechanism, we’ve introduced several architectural improvements that significantly enhance its performance.</p> <p>At a high level, DeltaNet follows the modern transformer block design popularized by Llama, alternating between token mixing (DeltaNet replacing self-attention) and channel mixing (SwiGLU). Our main architectural modifications focus on the token mixing layer, where we introduce three key improvements. First, we replace the original L₁ normalization and 1+ELU activation with L₂ normalization and SiLU activation for query and key processing. Second, we add short convolution operations after the linear projections for queries, keys, and values. Third, we incorporate output normalization before the final projection.</p> <p>The complete processing pipeline now follows this structure:</p> <ul> <li>Query/Key: Linear → ShortConv → SiLU → L₂Norm</li> <li>Value: Linear → ShortConv → SiLU</li> <li>Beta: Linear → Sigmoid</li> <li>Output: Delta rule(query, key, value, beta) → RMSNorm → Linear</li> </ul> <p>Let’s examine why each of these modifications proves crucial for model performance.</p> <h3 id="normalization-on-queries-and-keys">Normalization on Queries and Keys</h3> <p>A crucial aspect of DeltaNet’s architecture is the normalization of key vectors. This isn’t just a technical detail - it’s fundamental to the model’s stability and effectiveness. Consider DeltaNet’s core equation:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t\mathbf{k}_t^\top) + \mathbf{v}_t\mathbf{k}_t^\top\] <p>The stability of this recurrent system depends on the eigenvalues of its transition matrix \((\mathbf{I} - \beta_t \mathbf{k}_t\mathbf{k}_t^\top)\). This matrix has an elegant spectral structure:</p> <ul> <li>The eigenvalue in the direction of \(\mathbf{k}_t\) is \(1 - \beta_t\|\mathbf{k}_t\|^2\)</li> <li>All directions perpendicular to \(\mathbf{k}_t\) have eigenvalue 1</li> </ul> <p>For stable updates, we need all eigenvalues to have magnitude \(\leq 1\). Given \(0 \leq \beta_t \leq 1\), this requires \(\|\mathbf{k}_t\|^2 \leq 2\). While the original DeltaNet used L₁ normalization, we found L₂ normalization offers both better empirical performance and a more intuitive geometric interpretation: when \(\beta_t = 1\) and \(\|\mathbf{k}_t\|_2 = 1\), the matrix \(\mathbf{I} - \mathbf{k}_t\mathbf{k}_t^\top\) becomes a projection matrix that selectively erases information in the direction of \(\mathbf{k}_t\) while preserving all other directions.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/projection.png" alt="示例图片" style="width: 67%"> </div> </div> <p>The projection matrix has an important geometric effect: when applied to any vector, it removes the component parallel to \(\mathbf{k}_t\) while preserving all orthogonal components. In the context of DeltaNet, this means each update “cleans up” the state by removing components that might interfere with the current key’s direction. This operation helps maintain cleaner separations between different key vectors over time, reducing the interference between stored patterns (or the retrieval error) that we discussed in the first post. This geometric property helps explain why L₂ normalization, which directly aligns with this projection interpretation, leads to better retrieval performance than L₁ normalization.</p> <p>We also find that applying L₂ normalization to queries improves model performance. This observation aligns with recent trends in self-attention architectures, where QK-normalization has emerged as an effective technique for stabilizing and enhancing attention mechanisms.</p> <p>Finally, we note a potential limitation in our current design: our transition matrices are constrained to have strictly positive eigenvalues. A recent insightful work<d-cite key="grazzi2024unlockingstatetrackinglinearrnns"></d-cite> demonstrates how this could limit the model’s state tracking capabilities. Fortunately, their proposed enhancement is remarkably simple - by adjusting our beta term to \(\beta_t = 2\beta_t\), we can allow for negative eigenvalues in our transition matrices. This one-line modification could meaningfully expand DeltaNet’s representational capabilities. We direct interested readers to the following discussion for a deeper analysis of this enhancement.</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">LLMs can now track states, finally matching this cat!<br>And we prove it.<br><br>But how? 🧵👇<br><br>1/ Paper: <a href="https://t.co/aKvrqYtkWh" rel="external nofollow noopener" target="_blank">https://t.co/aKvrqYtkWh</a><br>with <a href="https://twitter.com/julien_siems?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@julien_siems</a> <a href="https://twitter.com/jkhfranke?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@jkhfranke</a> <a href="https://twitter.com/ZelaArber?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@ZelaArber</a>  <a href="https://twitter.com/FrankRHutter?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@FrankRHutter</a>   <a href="https://twitter.com/MPontil?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@MPontil</a> <a href="https://t.co/2OREoLkDyY" rel="external nofollow noopener" target="_blank">pic.twitter.com/2OREoLkDyY</a></p>— Riccardo Grazzi (@riccardograzzi) <a href="https://twitter.com/riccardograzzi/status/1860017064473428220?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">November 22, 2024</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h3 id="normalization-on-outputs">Normalization on Outputs</h3> <p>In standard linear attention, the output at each position is normalized by the sum of attention weights:</p> \[\mathbf{o}_t = \frac{(\sum_{i=1}^t \mathbf{v}_i \phi(\mathbf{k})_i^\top)\phi(\mathbf{q})_t}{\sum_{i=1}^t \phi(\mathbf{k})_i^\top \phi(\mathbf{q})_t}\] <p>where \(\phi\) is a positive feature map. However, a seminal analysis by Qin et al. <d-cite key="qin_devil_2022"></d-cite> demonstrates that this normalization term can lead to unbounded gradients and training instability. To address this issue, they propose removing the denominator and instead applying normalization to the output before the final projection. This architectural modification has since become standard practice, adopted by modern linear attention models including RetNet, GLA, Mamba2, and others.</p> <h3 id="activation-function-choice">Activation Function Choice</h3> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/silu.png" alt="示例图片" style="width: 50%"> </div> </div> <p>While the original DeltaNet used 1+ELU activation, our experiments show that SiLU activation provides better performance, a finding aligned with recent architectures like Mamba2<d-cite key="mamba2"></d-cite>, xLSTM<d-cite key="beck_xlstm_2024"></d-cite>, and LightningAttention <d-cite key="Qin2024VariousLC"></d-cite>. This presents an interesting contrast with traditional linear attention models, which typically choose activation functions that ensure positive attention scores through positive feature maps (e.g., ReLU, 1+ELU, exponential). The success of allowing negative values parallels findings from Differential Transformers <d-cite key="ye2024differentialtransformer"></d-cite>, suggesting that restricting attention scores to be strictly positive may be unnecessarily limiting.</p> <h3 id="short-convolution">Short Convolution</h3> <p>Short convolution<d-cite key="Poli2023HyenaHT"></d-cite>, i.e., depthwise separable Conv1D with small kernel window size as small as 4, has emerged as a crucial component in recent subquadratic attention models, appearing in various forms across architectures like Mamba, xLSTM<d-cite key="beck_xlstm_2024"></d-cite>, and MetaLA<d-cite key="chou2024metala"></d-cite>. This generalizes the previously proposed “shift-SSM” in H3<d-cite key="h3"></d-cite> and has a close relationship to “token-shift” in RWKV<d-cite key="peng-etal-2023-rwkv"></d-cite>.</p> <p>While DeltaNet’s delta rule excels at content-based interactions, many linguistic patterns require precise positional information. Short convolution provides direct access to local context, enabling the model to capture position-dependent patterns without relying on content-based matching. This combination of content-based and position-sensitive processing has proven highly effective across modern architectures.</p> <h3 id="experimental-results">Experimental Results</h3> <p>With the parallel algorithm in hand, and with the architecture above, we are now ready to scale up DeltaNet to standard language modeling settings. Our evaluation spans three key metrics: language modeling (WikiText perplexity), common-sense reasoning (averaged across LAMBADA, PiQA, HellaSwag, WinoGrande, ARC-easy, and ARC-challenge), and in-context retrieval (averaged across FDA, SWDE, and SQuAD).</p> <p>Regarding state size across architectures (H denotes number of layers, d denotes model dimension):</p> <table> <thead> <tr> <th>Architecture</th> <th>State Expansion</th> <th>Total State Size</th> <th>Implementation Details</th> </tr> </thead> <tbody> <tr> <td><strong>Mamba</strong></td> <td>16x</td> <td>64Hd</td> <td>Expands value projections to 2d and uses 16x expansion ratio; doubles effective state size by replacing FFN with Mamba layers</td> </tr> <tr> <td><strong>RetNet</strong></td> <td>512x</td> <td>512Hd</td> <td>Expands value projections to 2d; maintains fixed 256-dimensional query/key heads</td> </tr> <tr> <td><strong>GLA</strong></td> <td>256x</td> <td>256Hd</td> <td>Uses half-sized query/key heads relative to value heads; maintains 4d² parameters per layer</td> </tr> <tr> <td><strong>DeltaNet</strong></td> <td>128x</td> <td>128Hd</td> <td>Employs consistent 128-dimensional heads throughout the architecture</td> </tr> </tbody> </table> <h4 id="main-results-340m15b-token">Main Results (340M+15B token)</h4> <table> <thead> <tr> <th>Model</th> <th>Wiki. ppl ↓</th> <th>Avg. Common-sense ↑</th> <th>Avg. Retrieval ↑</th> <th>State Size</th> </tr> </thead> <tbody> <tr> <td>Transformer++</td> <td>28.39</td> <td>41.2</td> <td>28.6</td> <td>N/A</td> </tr> <tr> <td>RetNet (w/o conv)</td> <td>32.33</td> <td>41.0</td> <td>14.6</td> <td>512x</td> </tr> <tr> <td>Mamba (w. conv)</td> <td>28.39</td> <td>41.8</td> <td>12.5</td> <td>64x</td> </tr> <tr> <td>GLA (w/o conv)</td> <td>28.65</td> <td>41.5</td> <td>18.0</td> <td>128x</td> </tr> <tr> <td>DeltaNet (w. conv)</td> <td>28.24</td> <td>42.1</td> <td>22.7</td> <td>128x</td> </tr> </tbody> </table> <p>DeltaNet achieves competitive performance across all metrics while maintaining reasonable state size requirements. Notably, it shows particular strength in retrieval tasks, supporting our hypothesis that its delta rule mechanism provides effective in-context retrieval capabilities.</p> <h4 id="ablation-study-340m15b-token">Ablation Study (340M+15B token)</h4> <table> <thead> <tr> <th>Model</th> <th>Wiki. ppl ↓</th> <th>Common-sense ↑</th> <th>Retrieval ↑</th> </tr> </thead> <tbody> <tr> <td>DeltaNet (full)</td> <td>28.24</td> <td>42.1</td> <td>22.7</td> </tr> <tr> <td>- w/o short conv</td> <td>29.08</td> <td>41.4</td> <td>18.6</td> </tr> <tr> <td>- w. \(L_1\)-norm + 1+ELU</td> <td>31.12</td> <td>40.1</td> <td>11.5</td> </tr> <tr> <td>- w. \(L_2\)-norm + 1+ELU</td> <td>28.03</td> <td>42.1</td> <td>21.8</td> </tr> <tr> <td>- w. \(L_2\)-norm + ReLU</td> <td>28.75</td> <td>40.9</td> <td>21.0</td> </tr> </tbody> </table> <p>Our ablation studies highlight several important findings about DeltaNet’s architecture. Most significantly, retrieval performance shows strong sensitivity to the choice of normalization - \(L_2\) normalization substantially outperforms \(L_1\) normalization, supporting our theoretical analysis about projection properties. Short convolution also emerges as a crucial component, demonstrating that effective position-based addressing meaningfully complements DeltaNet’s content-based mechanism for retrieval tasks. The choice of activation function, while still relevant, shows more modest effects; SiLU provides incremental improvements over ReLU and 1+ELU, but its impact is less pronounced than either normalization or short convolution.</p> <h2 id="hybrid-models-combining-deltanet-with-attention">Hybrid Models: Combining DeltaNet with Attention</h2> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/hybrid.png" alt="示例图片" style="width: 99%"> <figcaption style="margin-top: 10px; color: #666;"> Left: Hybrid sliding window attention and DeltaNet model. Right: Hybrid global attention and DeltaNet model. </figcaption> </div> </div> <p>While DeltaNet’s delta rule mechanism shows promise for retrieval tasks, it still faces a fundamental limitation common to all RNN architectures: fixed state size. This constraint creates an inherent ceiling for retrieval performance, regardless of the choice of update rule<d-cite key="wen_rnns_2024"></d-cite>. To overcome this limitation, we explore hybrid architectures that strategically combine DeltaNet with attention mechanisms.</p> <p>Our first approach integrates sliding window attention with DeltaNet in an interleaving pattern, following recent architectures like Griffin<d-cite key="de_griffin_2024"></d-cite> and Samba<d-cite key="ren2024samba"></d-cite>. While this hybrid maintains subquadratic complexity due to the fixed window size, it inherits similar theoretical constraints as pure RNN models. As demonstrated in Griffin<d-cite key="de_griffin_2024"></d-cite>, the fixed context window can limit the retrieval of information beyond its scope.</p> <p>This limitation led us to our second approach: augmenting DeltaNet with global attention. Rather than replacing many DeltaNet layers with attention, which would significantly impact inference efficiency, we choose to place just two global attention layers - one in the second layer and another at layer N/2-1, following H3<d-cite key="h3"></d-cite>. Though this technically makes the model no longer subquadratic, the sparing use of attention layers substantially reduces KV cache requirements compared to full Transformer models.</p> <p>Results at the 340M parameter scale demonstrate the effectiveness of these hybrid approaches:</p> <table> <thead> <tr> <th>Model</th> <th>Wiki. ppl ↓</th> <th>Avg. Common-sense ↑</th> <th>Avg. Retrieval ↑</th> </tr> </thead> <tbody> <tr> <td>Transformer++</td> <td>28.39</td> <td>41.2</td> <td>28.6</td> </tr> <tr> <td>DeltaNet</td> <td>28.24</td> <td>42.1</td> <td>22.7</td> </tr> <tr> <td>+ Sliding Attn</td> <td>27.06</td> <td>42.1</td> <td>30.2</td> </tr> <tr> <td>+ Global Attn</td> <td>27.51</td> <td>42.1</td> <td>32.7</td> </tr> </tbody> </table> <p>We then scaled our experiments to 1.3B parameters, training for 100B tokens on SlimPajama. The results reinforce our findings:</p> <table> <thead> <tr> <th>Model</th> <th>Wiki. ppl ↓</th> <th>Avg. Common-sense ↑</th> <th>Avg. Retrieval ↑</th> </tr> </thead> <tbody> <tr> <td>Transformer++</td> <td>16.85</td> <td>50.9</td> <td>41.8</td> </tr> <tr> <td>DeltaNet</td> <td>16.87</td> <td>51.6</td> <td>34.7</td> </tr> <tr> <td>+ Sliding Attn</td> <td>16.56</td> <td>52.1</td> <td>39.6</td> </tr> <tr> <td>+ Global Attn</td> <td>16.55</td> <td>51.8</td> <td>47.9</td> </tr> </tbody> </table> <p>While sliding window attention provides substantial gains, it cannot fully match Transformer-level retrieval performance in larger scale. However, the addition of just two global attention layers<d-footnote>Recent work demonstrates that using global attention in only a small portion (~10%) of total layers can be highly effective for model performance <d-cite key="DBLP:journals/corr/abs-2406-07887,DBLP:journals/corr/abs-2403-19887"></d-cite>.</d-footnote> yields remarkable results, surpassing even the Transformer baseline in retrieval tasks.</p> <p>Finally, we evaluated a 3B parameter model trained on 1T tokens following the PowerLM-3B setup <d-cite key="Shen2024PowerSA"></d-cite>. These results place DeltaNet as a strong performer among RNN architectures while slightly trailing transformer-based models:</p> <table> <thead> <tr> <th>Model</th> <th>ARC</th> <th>HellaSwag</th> <th>OBQA</th> <th>PIQA</th> <th>WinoGrande</th> <th>MMLU</th> <th>Average</th> </tr> </thead> <tbody> <tr> <td>Llama-3.2-3B</td> <td>59.1</td> <td>73.6</td> <td>43.4</td> <td>77.5</td> <td>69.2</td> <td>54.1</td> <td>62.8</td> </tr> <tr> <td>PowerLM-3B</td> <td>60.5</td> <td>74.6</td> <td>43.6</td> <td>79.9</td> <td>70.0</td> <td>45.0</td> <td>62.3</td> </tr> <tr> <td>DeltaNet-3B</td> <td>60.4</td> <td>72.8</td> <td>41.0</td> <td>78.5</td> <td>65.7</td> <td>40.7</td> <td>59.8</td> </tr> <tr> <td>RecurrentGemma-2B</td> <td>57.0</td> <td>71.1</td> <td>42.0</td> <td>78.2</td> <td>67.6</td> <td>31.8</td> <td>57.9</td> </tr> <tr> <td>RWKV-6-3B</td> <td>49.5</td> <td>68.6</td> <td>40.6</td> <td>76.8</td> <td>65.4</td> <td>28.4</td> <td>54.9</td> </tr> <tr> <td>Mamba-2.7B</td> <td>50.3</td> <td>65.3</td> <td>39.4</td> <td>75.8</td> <td>63.1</td> <td>26.1</td> <td>53.3</td> </tr> </tbody> </table> <p>The results demonstrate DeltaNet’s effectiveness across scales, though there remains a small gap compared to transformer architectures at larger sizes. We are currently exploring larger hybrid models combining DeltaNet with attention mechanisms - stay tuned for updates!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-12-03-delta.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Songlin Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>