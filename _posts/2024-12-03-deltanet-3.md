---
layout: distill
title: DeltaNet Explained (Part III)
date: 2024-12-03 22:25:00
description: Modernize DeltaNet neural architecture 
featured: true

authors:
  - name: Songlin Yang
    url: "https://sustcsonglin.github.io/"
    affiliations:
      name: MIT CSAIL

bibliography: 2024-12-03-delta.bib


toc:
  - name: "DeltaNet architecture design"
    subsections:
      - name: Normalization on transition matrices
      - name: Activation Function Choice
      - name: The Critical Role of Short Convolution
      - name: Experimental Results
  - name: "Hybrid Models: Combining DeltaNet with Attention"
---


**This blog post series accompanies our NeurIPS '24 paper - <a href="https://arxiv.org/abs/2406.06484">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a>** (w/ [Bailin Wang](https://berlino.github.io/), [Yu Zhang](https://yzhang.site/), [Yikang Shen](https://mitibmwatsonailab.mit.edu/people/yikang-shen/) and [Yoon Kim](https://people.csail.mit.edu/yoonkim/)). **You can find the implementation [here](https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py) and the presentation slides [here](https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf).**

<ol>
    <li><a href="/blog/2024/deltanet-1/">Part I - The Model</a></li>
    <li><a href="/blog/2024/deltanet-2/">Part II - The Algorithm</a></li>
    <li><a href="#">Part III - The Neural Architecture</a></li>
</ol>

## DeltaNet architecture design

<div class="row" style="text-align: center">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/delta-net-arch.png" alt="示例图片" style="width: 99%"/>
            <figcaption style="margin-top: 10px; color: #666;">
                DeltaNet architecture
            </figcaption>
    </div>
</div>

In this final post of our series, we explore how we modernized DeltaNet with contemporary architectural choices, analyze why each component matters, and present empirical results demonstrating its effectiveness.

The overall model structure follows Llama's design, alternating between a token mixer (DeltaNet instead of self-attention) and a channel mixer (SwiGLU). Within the DeltaNet layer, queries and keys share the same processing pipeline: Linear → ShortConv → SiLU → L2Norm, while values follow Linear → ShortConv → SiLU, and the beta term is computed through Linear → Sigmoid. Let's first examine the motivation behind each of these design choices.


### Normalization on transition matrices

A crucial aspect of DeltaNet's architecture is the normalization of key vectors. This isn't just a technical detail - it's fundamental to the model's stability and effectiveness. Consider DeltaNet's core equation:

$$ \mathbf{S}_{t} = \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t\mathbf{k}_t^\top) + \mathbf{v}_t\mathbf{k}_t^\top$$

The stability of this recurrent system depends on the eigenvalues of its transition matrix $$(\mathbf{I} - \beta_t \mathbf{k}_t\mathbf{k}_t^\top)$$. This matrix has an elegant spectral structure:
- The eigenvalue in the direction of $$\mathbf{k}_t$$ is $$1 - \beta_t\|\mathbf{k}_t\|^2$$
- All directions perpendicular to $$\mathbf{k}_t$$ have eigenvalue 1

For stable updates, we need all eigenvalues to have magnitude $$\leq 1$$. Given $$0 \leq \beta_t \leq 1$$, this requires $$\|\mathbf{k}_t\|^2 \leq 2$$. While the original DeltaNet used L1 normalization, we found L2 normalization offers both better empirical performance and a more intuitive geometric interpretation: when $$\beta_t = 1$$ and $$\|\mathbf{k}_t\|_2 = 1$$, the matrix $$\mathbf{I} - \mathbf{k}_t\mathbf{k}_t^\top$$ becomes a projection matrix that selectively erases information in the direction of $$\mathbf{k}_t$$ while preserving all other directions.

<div class="row" style="text-align: center">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/projection.png" alt="示例图片" style="width: 67%"/>
    </div>
</div>

We also found that L2 normalization on queries improves performance, possibly related to the success of QK-normalization in recent attention models.

### Activation Function Choice

<div class="row" style="text-align: center">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/silu.png" alt="示例图片" style="width: 50%"/>
    </div>
</div>

Our experiments show SiLU activation performs slightly better than alternatives like ReLU and 1+ELU. This presents an interesting contrast with traditional linear attention mechanisms, where activation functions are typically chosen to ensure positive attention scores through positive feature maps. SiLU's ability to produce negative values actually appears beneficial in our case. This aligns with recent findings from works like Differential Transformers that demonstrate the value of allowing negative attention scores - suggesting that restricting attention to be strictly positive might be unnecessarily limiting. 


### The Critical Role of Short Convolution

Short convolution has emerged as a crucial component in recent subquadratic attention models, appearing in various forms across architectures like Mamba, xLSTM, and MetaLA. This pattern echoes earlier innovations like the "shift-SSM" in H3<d-cite key="h3"></d-cite> and "token-shift" in RWKV<d-cite key="peng-etal-2023-rwkv"></d-cite>. Our analysis reveals that DeltaNet similarly relies heavily on short convolution operations to enhance its performance. We can understand this dependency through the theoretical framework of Neural Turing Machines (NTM) <d-cite key="DBLP:journals/corr/GravesWD14"></d-cite>.
To appreciate this connection, let's first consider how NTMs handle information access. NTMs identified two fundamental addressing mechanisms: content-based addressing (finding information based on what it contains) and position-based addressing (accessing information based on where it's located). DeltaNet's core operation centers on content-based addressing, implemented through its state update rule:
$$\mathbf{S}t = \mathbf{S}{t-1}(\mathbf{I} - \beta_t\mathbf{k}_t\mathbf{k}_t^\top) + \beta_t\mathbf{v}_t\mathbf{k}_t^\top$$
This update mechanism allows DeltaNet to locate and modify information based on key similarity, analogous to how NTM's content addressing locates memory locations with similar content. While the sequential nature of these updates makes the system inherently order-sensitive (as each update affects the state seen by subsequent operations), the primary mechanism still operates through content similarity rather than explicit positional information.

However, many patterns in sequence processing require precise positional relationships. The NTM recognized this need and introduced convolution-based position addressing as a complementary mechanism to its content addressing. This allowed the NTM to shift attention by exact positions relative to its current focus, enabling operations like "move one step left" or "copy the next three elements."

Short convolution in DeltaNet serves a similar purpose - it provides deterministic access to recent positions, independent of content. This position-based addressing is essential for tasks like forming induction heads within a single layer, where a model needs to identify and copy patterns from specific relative positions (like "A B A B A ..."). While DeltaNet's content-based mechanism could theoretically learn such patterns, having direct positional access through convolution makes this much more reliable.

Think of content addressing as finding information by what it contains (like looking up a topic in a book's index) versus position addressing as finding information by where it sits (like knowing exactly which page to turn to). Both capabilities are necessary: content addressing for flexible, semantic relationships, and position addressing for precise, structural patterns. This duality, first recognized in the NTM's design, remains crucial for modern architectures like DeltaNet. 


### Experimental Results

#### Main Results (340M)

| Model | Wiki. ppl ↓ | Avg. Common-sense  ↑ | Avg. Retrieval ↑| State Size |
|-------|-------------|------------------------------|-----------------|-------------|
| Transformer++ | 28.39 | 41.2 | 28.6 | N/A |
| RetNet (w/o conv) | 32.33 | 41.0 | 14.6 | 512x |
| Mamba (w. conv) | 28.39 | 41.8 | 12.5 | 64x |
| GLA (w/o conv) | 28.65 | 41.5 | 18.0 | 128x |
| DeltaNet (w. conv) | 28.24 | 42.1 | 22.7 | 128x |

DeltaNet achieves competitive performance across all metrics while maintaining reasonable state size requirements. Notably, it shows particular strength in retrieval tasks, supporting our hypothesis that its delta rule mechanism provides effective in-context retrieval capabilities.

#### Ablation Study (340M)

| Model | Wiki. ppl ↓ | Common-sense ↑ | Retrieval ↑ | 
|---------------|---------|------------------------|------------|
| DeltaNet (full) | 28.24 | 42.1 | 22.7 |
| - w/o short conv | 29.08 | 41.4 | 18.6 | 
| - w. $$L_1$$-norm + 1+ELU | 31.12 | 40.1 | 11.5 |
| - w. $$L_2$$-norm + 1+ELU | 28.03 | 42.1 | 21.8 |
| - w. $$L_2$$-norm + ReLU | 28.75 | 40.9 | 21.0 |

Our ablation studies highlight several important findings about DeltaNet's architecture. Most significantly, retrieval performance shows strong sensitivity to the choice of normalization - $$L_2$$ normalization substantially outperforms $$L_1$$ normalization, supporting our theoretical analysis about projection properties. Short convolution also emerges as a crucial component, demonstrating that effective position-based addressing meaningfully complements DeltaNet's content-based mechanism for retrieval tasks. The choice of activation function, while still relevant, shows more modest effects; SiLU provides incremental improvements over ReLU and 1+ELU, but its impact is less pronounced than either normalization or short convolution.

## Hybrid Models: Combining DeltaNet with Attention

<div class="row" style="text-align: center">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/hybrid.png" alt="示例图片" style="width: 99%"/>
            <figcaption style="margin-top: 10px; color: #666;">
                Left: Hybrid sliding window attention and DeltaNet model. Right: Hybrid global attention and DeltaNet model. 
            </figcaption>
    </div>
</div>

While DeltaNet's delta rule mechanism shows promise for retrieval tasks, it still faces a fundamental limitation common to all RNN architectures: fixed state size. This constraint creates an inherent ceiling for retrieval performance, regardless of the choice of update rule<d-cite key="wen_rnns_2024"></d-cite>. To overcome this limitation, we explore hybrid architectures that strategically combine DeltaNet with attention mechanisms.

Our first approach integrates sliding window attention with DeltaNet in an interleaving pattern, following recent architectures like Griffin<d-cite key="de_griffin_2024"></d-cite> and Samba<d-cite key="ren2024samba"></d-cite>. While this hybrid maintains subquadratic complexity due to the fixed window size, it inherits similar theoretical constraints as pure RNN models. As demonstrated in Griffin<d-cite key="de_griffin_2024"></d-cite>, the fixed context window can limit the retrieval of information beyond its scope.

This limitation led us to our second approach: augmenting DeltaNet with global attention. Rather than replacing many DeltaNet layers with attention, which would significantly impact inference efficiency, we choose to place just two global attention layers - one in the second layer and another at layer N/2-1, following H3<d-cite key="h3"></d-cite>. Though this technically makes the model superquadratic, the sparing use of attention layers substantially reduces KV cache requirements compared to full Transformer models.

Results at the 1.3B parameter scale demonstrate the effectiveness of these hybrid approaches:

| Model | Wiki. ppl ↓ | Avg. Common-sense ↑ | Avg. Retrieval ↑ |
|--------------|-------------|---------------------|------------------|
| Transformer++ | 16.85 | 50.9 | 41.8 |
| DeltaNet | 16.87 | 51.6 | 34.7 |
| + Sliding Attn | 16.56 | 52.1 | 39.6 |
| + Global Attn | 16.55 | 51.8 | 47.9 |

While sliding window attention provides substantial gains, it cannot fully match Transformer-level retrieval performance. However, the addition of just two global attention layers<d-footnote>Recent work demonstrates that using global attention in only a small portion (~10%) of total layers can be highly effective for model performance <d-cite key="DBLP:journals/corr/abs-2406-07887,DBLP:journals/corr/abs-2403-19887"></d-cite>.</d-footnote> yields remarkable results, surpassing even the Transformer baseline in retrieval tasks.
