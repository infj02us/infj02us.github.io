---
layout: distill
title: DeltaNet Explained (Part II)
date: 2024-12-03 22:25:00
description: An algorithm that parallelizes DeltaNet computation across the sequence length dimension
featured: true


authors:
  - name: Songlin Yang
    url: "https://sustcsonglin.github.io/"
    affiliations:
      name: MIT CSAIL

bibliography: 2024-12-03-delta.bib

toc:
  - name: "Parallel Scan for DeltaNet: A Failed Attempt"
  - name: WY representation for DeltaNet

---

**This blog post series accompanies our NeurIPS '24 paper - <a href="https://arxiv.org/abs/2406.06484">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a>. You can find the implementation [here](https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py) and the presentation slides [here](https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf).**

<ol>
    <li><a href="/blog/2024/deltanet-1/">Part I - The Model</a></li>
    <li><a href="#">Part II - The Algorithm</a></li>
</ol>

<!-- A reminder, in last post, we have the DeltaNet equations and we do an additonal algrebra step to restructure it as a matrix-mulitply form.

$$

\begin{align*}

\mathbf{S}_{t} &= \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top \\

&= \mathbf{S}_{t-1} - \beta_t \mathbf{S}_{t-1} \mathbf{k}_t \mathbf{k}_t^\top + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \\

&= \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal) + \beta_t \mathbf{v}_t \mathbf{k}_t^\top

\end{align*}

$$


It is known that such linear recurrence could be parallelzied across sequence length

$$
   \rmS_t =   \rmS_{t-1} \bullet \rmM_t  + \vv_t  \vk_t^\intercal
$$

for arbitrary associative operator. In a classical mateiral? , Section 3.2, Lemma says


 -->


## Parallel Scan for DeltaNet: A Failed Attempt

### From Delta Updates to Matrix Multiplication Form

Let's start with DeltaNet's original state update equation:

$$
\mathbf{S}_{t} = \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top
$$

To transform this into a matrix multiplication form, let's expand the equation step by step:

$$
\begin{align*}
\mathbf{S}_{t} &= \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top \\
&= \mathbf{S}_{t-1} - \beta_t \mathbf{S}_{t-1} \mathbf{k}_t \mathbf{k}_t^\top + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \\
&= \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal) + \beta_t \mathbf{v}_t \mathbf{k}_t^\top
\end{align*}
$$

For simplicity, let's denote:
- $$\mathbf{M}_t = \mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal$$ as our transition matrix
- $$\mathbf{X}_t = \beta_t \mathbf{v}_t \mathbf{k}_t^\top$$ as our update term

Then our update becomes:

$$
\mathbf{S}_{t} = \mathbf{S}_{t-1}\mathbf{M}_t + \mathbf{X}_t \in \mathbb{R}^{d\times d}
$$

### Defining the Associative Operator

This form matches exactly with the first-order recurrence shown in equation (1.5) from *Prefix Sums
and Their Applications*<d-cite key="Blelloch1990PrefixSA"></d-cite>, where matrix multiplication (⊗) and matrix addition (⊕) serve as our binary operators. Both operators satisfy the required properties:

1. Matrix addition is associative: $$(A + B) + C = A + (B + C)$$
2. Matrix multiplication is associative: $$(AB)C = A(BC)$$
3. Matrix multiplication distributes over addition: $$A(B + C) = AB + AC$$

Following the framework, we define our state pairs as:

$$c_t = [\mathbf{M}_t, \mathbf{X}_t] = [\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal, \beta_t \mathbf{v}_t \mathbf{k}_t^\top]$$

And our associative operator • that combines these pairs:

$$c_i \bullet c_j = [\mathbf{M}_i\mathbf{M}_j, \mathbf{M}_j\mathbf{X}_i + \mathbf{X}_j]$$

This operator definition preserves the temporal dependencies of our updates - when we combine two steps, the earlier update term $$\mathbf{X}_i$$ must be transformed by the later transition matrix $$\mathbf{M}_j$$, while the later update term $$\mathbf{X}_j$$ remains unchanged. 

### Parallel Scan

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/scan.png" alt="示例图片" style="width: 99%"/>
    </div>
</div>

With this associative operator, we can use parallel scan to compute all states in parallel. The algorithm works in two phases:

##### Sweep-Down Phase

First, we compute partial results in parallel by combining adjacent pairs:

For steps 0 and 1, we compute:

$$c_1 = c_0 \bullet c_1 = [\mathbf{M}_0\mathbf{M}_1, \mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1]$$

Similarly for steps 2 and 3:

$$c_3 = c_2 \bullet c_3 = [\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]$$

Then combine these results:

$$c_{1:3} = c_{1} \bullet c_{3} = [\mathbf{M}_0\mathbf{M}_1\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_2\mathbf{M}_3(\mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1) + \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]$$

##### Sweep-Up Phase

In this phase, we use our partial results to compute intermediate states:

$$c_2 = c_1 \bullet c_2 = [\mathbf{M}_1\mathbf{M}_2, \mathbf{M}_2\mathbf{X}_1 + \mathbf{X}_2]$$

This parallelization transforms DeltaNet's sequential state updates into an efficient parallel computation, reducing the sequential dependency chain from $$\mathbf{O}(L)$$ to $$\mathcal{O}(\log L)$$ steps while maintaining mathematical equivalence. 

### What's Wrong with Parallel Scan for DeltaNet?

Despite parallelizability, parallel scan for DeltaNet faces two major challenges: computational complexity and memory requirements.

The first issue lies in the **time complexity**. For DeltaNet, parallel scan yields $$\mathcal{O}(L\log L d^3)$$ complexity due to the cubic cost of matrix multiplication when treating $$\mathbf{M}_t$$ as dense matrices. At first glance, we might think we can leverage the identity-plus-low-rank structure of $$\mathbf{M}_t$$ for acceleration. Let's work through this carefully.

When multiplying two adjacent matrices, we get:

$$

\begin{align*}

(\mathbf{I}-\beta_0 \mathbf{k}_0 \mathbf{k}_0^\top)(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) 

&= \mathbf{I}(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) \\

&= (\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 \mathbf{k}_0^\top \mathbf{k}_1 \mathbf{k}_1^\top \\

&= \mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top

\end{align*}

$$

This computation reduces the complexity from $$\mathcal{O}(d^3)$$ to $$\mathcal{O}(d^2)$$ by leveraging the identity-plus-low-rank structure - we only need to compute vector inner products $$(\mathbf{k}_0^\top \mathbf{k}_1)$$ and outer products between vectors. Similarly for the next pair:

$$

\begin{align*}

(\mathbf{I}-\beta_2 \mathbf{k}_2 \mathbf{k}_2^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top) 

&= \mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top

\end{align*}

$$

When we try to combine these results to compute larger spans like $$c_{1:4}$$, the multiplication becomes increasingly complex. We need to multiply:

$$(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top)$$

Each term in the first bracket must multiply with each term in the second bracket, leading to a quadratic growth in the number of terms. This quickly becomes unmanageable due to the combinatorial explosion of terms. This suggests we might be better off just treating it as dense matrix multiplication.

The second major issue is **space complexity**. Parallel scan requires materializing all intermediate d×d matrices at each step to high-bandwidth memory (HBM). For linear RNNs with matrix-valued states, this materialization becomes prohibitively expensive ($$\mathcal{O}(Ld^2)$$). While recurrent computation can avoid such materialization, parallel scan offers no apparent workaround unless all states fit into SRAM, as implemented in Mamba's hardware-aware selective scan algorithm that eliminates the need for materialization. However, this approach imposes limitations on state size - too large a state leads to out-of-shared-memory issues. Given that I/O costs dominate this computation, parallel scan may become undesirable in practice.
As noted in recent discussions:

{% twitter https://x.com/francoisfleuret/status/1793016689589625263 %}

{% twitter https://x.com/SonglinYang4/status/1793029555277697379 %}

Here I previously discussed the chunkwise algorithm - another type of associative scan that offers improved memory efficiency and better utilization of tensor cores by enabling more matrix multiplication operations (for a detailed analysis, see <d-cite key="yang_gated_2023"></d-cite>). Given these advantages, developing a chunkwise training algorithm for DeltaNet that maintains quadratic complexity with respect to $$d$$ while preserving memory efficiency would be highly valuable.

## WY representation for DeltaNet

Stay Tuned!